import { Callout } from "nextra-theme-docs";

<Callout type="warning" emoji="⚠️">
  This documentation is under construction and incomplete.
</Callout>

# Reinforcement Learning (RL)

## PPO

We use the Proximal Policy Optimization (PPO) algorithm for training our humanoid robots.

Proximal policy optimization (PPO) is an on-policy reinforcement-learning algorithm known for stability and sample-efficiency. It works by limiting how far the policy can stray from the current policy, aiding stability. A common issue with RL, especially on-policy RL—where the learning algorithm can only use current observations to inform policy updates—is that algorithms tend to be myopic, in that they aggressively optimize for the temporary environment of the current policy, to the detriment of stability. PPO improves upon previous policy-gradient methods by introducing a clipped surrogate objective, which limits the extent to which policies can deviate from previous policies.

PPO is particularly suited for humanoid robotics. It is simple, stable, sample efficient, and uses continuous action spaces.

Our PPO implementation follows this high-level algorithm:

```
While not done:
    Collect experiences using current policy
    If update interval reached:
        Compute advantages and returns
        For each epoch:
            For each mini-batch:
                Compute policy ratio r = new*policy / old_policy
                Calculate L_clip = min(r * advantage, clip(r, 1-ε, 1+ε) _ advantage)
                Compute value loss L_vf and entropy bonus
                Loss = -L_clip + c1 _ L*vf - c2 * entropy
                Perform gradient update
        Clear experience buffer
```

Currently, our hyper parameters are:

- Learning rate: 1e-5 (adaptive)
- Entropy coefficient: 0.001
- Discount factor (gamma): 0.994
- GAE lambda: 0.9
- Number of epochs: 2
- Number of mini-batches: 4
- Actor network: [512, 256, 128] hidden units
- Critic network: [768, 256, 128] hidden units

In our initial experiments, the PPO policy successfully learned basic locomotion behaviours. See `#Traning and Experiments` for details.

### Standing Rewards

The full standing reward function for our humanoid robot is a weighted sum of multiple components, each designed to encourage specific behaviours or penalize undesired ones. The total reward $R_{standing}$ is calculated as:

$$
\begin{align*}
R_{standing} &= w_{\text{joint\_pos}} r_{\text{joint\_pos}} + w_{\text{feet\_distance}} r_{\text{feet\_distance}} + w_{\text{knee\_distance}} r_{\text{knee\_distance}} + w_{\text{foot\_slip}} r_{\text{foot\_slip}} \\
  &+ w_{\text{feet\_air\_time}} r_{\text{feet\_air\_time}} + w_{\text{feet\_contact\_number}} r_{\text{feet\_contact\_number}} + w_{\text{orientation}} r_{\text{orientation}} \\
  &+ w_{\text{feet\_contact\_forces}} r_{\text{feet\_contact\_forces}} + w_{\text{default\_joint\_pos}} r_{\text{default\_joint\_pos}} + w_{\text{base\_height}} r_{\text{base\_height}} \\
  &+ w_{\text{base\_acc}} r_{\text{base\_acc}} + w_{\text{vel\_mismatch}} r_{\text{vel\_mismatch}} + w_{\text{torques}} r_{\text{torques}} \\
  &+ w_{\text{dof\_vel}} r_{\text{dof\_vel}} + w_{\text{dof\_acc}} r_{\text{dof\_acc}} + w_{\text{collision}} r_{\text{collision}} \\
  &+ w_{\text{action\_smoothness}} r_{\text{action\_smoothness}}
\end{align*}
$$

where $w_i$ is the weight for each reward component $r_i$, defined in the configuration scales.

<details>
  <summary>Show Reward Component Equations</summary>

todo

</details>

### Walking Rewards

For training the robot to walk, we have an additional set of rewards that are added to the standing rewards:

$$
R_\text{walking} = R_{standing} + w_{\text{feet\_clearance}} r_{\text{feet\_clearance}} + w_{\text{track\_vel}} r_{\text{track\_vel}} + w_{\text{low\_speed}} r_{\text{low\_speed}}
$$

where $w_i$ is the weight for each additional reward component $r_i$, also defined in the configuration scales.

<details>
  <summary>Show Walking Reward Component Equations</summary>

todo

</details>
